{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2gZSoSJD5C"
      },
      "source": [
        "# Verified Integer Mathematics in Transformers - Analyze the Model\n",
        "\n",
        "This Colab analyzes the behavior and algorithm sub-tasks performed by nodes in Transformer models.\n",
        "\n",
        "The models perform integer addition and/or subtraction e.g. 133357+182243=+0315600 and 123450-345670=-0123230. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens.\n",
        "\n",
        "This Colab follows on from https://github.com/PhilipQuirke/verified_transformers/blob/main/notebooks/VerifiedArithmeticTrain.ipynb which trained the models, and outputs model_name.pth and model_name_train.json\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkGrSqHJKqN"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd3nmsMJV5T"
      },
      "source": [
        "# Part 0: Import libraries\n",
        "Imports standard libraries.\n",
        "\n",
        "Imports \"verified_transformer\" public library as \"qt\". This library is specific to this CoLab's \"QuantaTool\" approach to transformer analysis. Refer to [README.md](https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCdmr6-_Jkzi",
        "outputId": "d536082a-30a7-49c5-fba0-f274bd91c6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.1.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.23.0 (from transformer_lens)\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer_lens)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.30-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.4)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.41.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.17.2-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.15.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.5)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer_lens)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.6.0-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, gitpython, wandb, datasets, accelerate, transformer_lens\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.31.0 beartype-0.14.1 better-abc-0.0.3 datasets-2.20.0 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.0 fancy-einsum-0.0.3 gitdb-4.0.11 gitpython-3.1.43 jaxtyping-0.2.30 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 sentry-sdk-2.6.0 setproctitle-1.3.3 smmap-5.0.1 transformer_lens-2.1.0 typeguard-2.13.3 wandb-0.17.2 xxhash-3.4.1\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.3.0+cu121)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torchtyping) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Installing collected packages: torchtyping\n",
            "Successfully installed torchtyping-0.1.4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    !pip install matplotlib\n",
        "\n",
        "    !pip install kaleido\n",
        "    !pip install transformer_lens\n",
        "    !pip install torchtyping\n",
        "    !pip install transformers\n",
        "\n",
        "    !pip install numpy\n",
        "    !pip install scikit-learn\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "    def setup_jupyter(install_libraries=False):\n",
        "        if install_libraries:\n",
        "            !pip install matplotlib==3.8.4\n",
        "            !pip install kaleido==0.2.1\n",
        "            !pip install transformer_lens==1.15.0\n",
        "            !pip install torchtyping==0.1.4\n",
        "            !pip install transformers==4.39.3\n",
        "\n",
        "            !pip install numpy==1.26.4\n",
        "            !pip install plotly==5.20.0\n",
        "            !pip install pytest==8.1.1\n",
        "            !pip install scikit-learn==1.4.1.post1\n",
        "\n",
        "        print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "        from IPython import get_ipython\n",
        "\n",
        "        ipython = get_ipython()\n",
        "        # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "        ipython.magic(\"load_ext autoreload\")\n",
        "        ipython.magic(\"autoreload 2\")\n",
        "\n",
        "    # setup_jupyter(install_libraries=True)   # Uncomment if you need to install libraries in notebook.\n",
        "    setup_jupyter(install_libraries=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Up2QLAZLJnG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e1c3ef-a805-4ba2-e4d4-688842d1508a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using renderer: colab\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ve-TndERJoaJ"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_6zOEFryJqGN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import re\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d6TE7A9SxySA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f8VQ4e0QJsIB"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gUt2mMzFj7eA"
      },
      "outputs": [],
      "source": [
        "# Import Principal Component Analysis (PCA) library\n",
        "pca_lib_avail = True\n",
        "try:\n",
        "  from sklearn.decomposition import PCA\n",
        "except Exception as e:\n",
        "  print(\"pca import failed with exception:\", e)\n",
        "  pca_lib_avail = False\n",
        "\n",
        "  # Sometimes version conflicts means the PCA library does not import. This workaround partially fixes the issue\n",
        "  !pip install --upgrade numpy\n",
        "  !pip install --upgrade scikit-learn\n",
        "\n",
        "  # To complete workaround, now select menu option \"Runtime > Restart session and Run all\".\n",
        "  stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zwekwkrdI6SX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfdaab6-beb1-4f9a-bf83-689def59624f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping QuantaTools as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip uninstall QuantaTools -y || true   # Ensure a clean install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2P3cndolKDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853c3f97-cf8c-47eb-c0dc-1d1ab6dabcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PhilipQuirke/verified_transformers.git\n",
            "  Cloning https://github.com/PhilipQuirke/verified_transformers.git to /tmp/pip-req-build-jkmx0arx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PhilipQuirke/verified_transformers.git /tmp/pip-req-build-jkmx0arx\n",
            "  Resolved https://github.com/PhilipQuirke/verified_transformers.git to commit add1e1ffd9eb46f9d3651d332e2897c66207e7f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from QuantaTools==0.1) (1.25.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from QuantaTools==0.1) (0.43.0)\n",
            "Building wheels for collected packages: QuantaTools\n",
            "  Building wheel for QuantaTools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for QuantaTools: filename=QuantaTools-0.1-py3-none-any.whl size=65363 sha256=de5a1369064eb44f4dfe64056349d1b078b729969d5dd7a63800190db3c64a4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmhdv28p/wheels/08/5c/b6/f02e46eb3b254e4572204214b92209ffbe2d0d4a5a61d3adb1\n",
            "Successfully built QuantaTools\n"
          ]
        }
      ],
      "source": [
        "# Refer https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md\n",
        "!pip install --upgrade git+https://github.com/PhilipQuirke/verified_transformers.git  # Specify @branch if testing a specific branch\n",
        "import QuantaTools as qt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGPkaokJQM5"
      },
      "source": [
        "# Part 1A: Configuration\n",
        "\n",
        "Which existing model do we want to analyze?\n",
        "\n",
        "The existing model weightings created by the sister Colab [VerifiedArithmeticTrain](https://github.com/PhilipQuirke/transformer-maths/blob/main/assets/VerifiedArithmeticTrain.ipynb) are loaded from HuggingFace (in Part 5). Refer https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DXZQ2E6yAi"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"main\" configuration class. MathsConfig is derived from the chain AlgoConfig > UsefulConfig > ModelConfig\n",
        "cfg = qt.MathsConfig()\n",
        "\n",
        "\n",
        "# Which model do we want to analyze? Uncomment one line:\n",
        "\n",
        "#cfg.model_name = \"\" # Use default configuration specified in cfg\n",
        "\n",
        "# Addition models\n",
        "cfg.model_name = \"add_d5_l1_h3_t15K_s372001\"  # AddAccuracy=Two9s. Inaccurate as only has one layer. Can predict S0, S1 and S2 complexity questions.\n",
        "#cfg.model_name = \"add_d5_l2_h3_t15K_s372001\"  # AddAccuracy=Six9s. AvgFinalLoss=1.6e-08\n",
        "#cfg.model_name = \"add_d5_l2_h3_t40K_s372001\"  # AddAccuracy=Six9s. AvgFinalLoss=2e-09. Fewest nodes\n",
        "#cfg.model_name = \"add_d6_l2_h3_t15K_s372001\"  # AddAccuracy=Fives. AvgFinalLoss=1.7e-08. (2/M fail: 018539+789353=+0807892 ModelAnswer: +0707892, 747332+057349=+0804681 ModelAnswer: +0704681)\n",
        "#cfg.model_name = \"add_d6_l2_h3_t20K_s173289\"  # AddAccuracy=Six9s. AvgFinalLoss=1.5e-08. Fewest nodes\n",
        "#cfg.model_name = \"add_d6_l2_h3_t20K_s572091\"  # AddAccuracy=Six9s. AvgFinalLoss=7e-09\n",
        "#cfg.model_name = \"add_d6_l2_h3_t40K_s372001\"  # AddAccuracy=Six9s. AvgFinalLoss 2e-09\n",
        "#cfg.model_name = \"add_d10_l2_h3_t40K_s572091\" # AddAccuracy=Six9s. AvgFinalLoss=7e-09. (1/M fail: 0000000555+0000000445=+00000001000 ModelAnswer: +00000000900)\n",
        "#cfg.model_name = \"add_d10_l2_h3_t40K_gf_s572091\" # AddAccuracy=Six9s. AvgFinalLoss=3.5-09. GrokFast.\n",
        "\n",
        "# Subtraction model\n",
        "#cfg.model_name = \"sub_d6_l2_h3_t30K_s372001\"  # SubAccuracy=Six9s. AvgFinalLoss=5.8e-06\n",
        "#cfg.model_name = \"sub_d10_l2_h3_t75K_s173289\"  # SubAccuracy=Two9s. (6672/M fails) AvgFinalLoss=0.002002022\n",
        "\n",
        "# Mixed (addition and subtraction) model\n",
        "#cfg.model_name = \"mix_d6_l3_h4_t40K_s372001\"  # Add/SubAccuracy=Six9s/Six9s. AvgFinalLoss=5e-09. (1/M fail: 463687+166096=+0629783 ModelAnswer: +0639783)\n",
        "#cfg.model_name = \"mix_d10_l3_h4_t75K_s173289\"  # Add/SubAccuracy=Five9s/Two9s. AvgFinalLoss=1.125e-06 (2/M fail: 3301956441+6198944455=+09500900896 ModelAnswer: +09500800896) (295/M fail: 8531063649-0531031548=+08000032101 ModelAnswer: +07900032101)\n",
        "\n",
        "# Mixed models initialized with addition model\n",
        "#cfg.model_name = \"ins1_mix_d6_l2_h3_t40K_s572091\"  # Add/SubAccuracy=Six9s/Five9s. AvgLoss = 2.4e-08 (5/M fails e.g. 565000-364538=+0200462 ModelAnswer: +0100462)\n",
        "#cfg.model_name = \"ins1_mix_d6_l3_h3_t40K_s572091\"  # Add/SubAccuracy=Six9s/Five9s. AvgFinalLoss=1.8e-08. (3/M fails e.g. 072074-272074=-0200000 ModelAnswer: +0200000)\n",
        "#cfg.model_name = \"ins1_mix_d6_l3_h3_t80K_s572091\"  # Add/SubAccuracy=Six9s/Five9s AvgLoss = 1.6e-08 (3/M fails e.g. 229672-229678=-0000006 ModelAnswer: +0000006) (EnrichFalse => 0/M, 4/M)\n",
        "#cfg.model_name = \"ins1_mix_d6_l3_h4_t40K_s372001\"  # Add/SubAccuracy=Six9s/Six9s. AvgFinalLoss=8e-09. MAIN FOCUS\n",
        "#cfg.model_name = \"ins1_mix_d6_l3_h4_t40K_s173289\"  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=1.4e-08. (3/M fails e.g. 850038+159060=+1009098 ModelAnswer: +0009098) (2/M fails e.g. 77285-477285=+0100000 Q: ModelAnswer: +0000000) (EnrichFalse => 0/M, 3/M)\n",
        "#cfg.model_name = \"ins1_mix_d6_l3_h4_t50K_s572091\"  # Add/SubAccuracy=Six9s/Five9s. AvgFinalLoss=2.9e-08. (4/M fails e.g. 986887-286887=+0700000 ModelAnswer: +0600000) (EnrichFalse => 0/M, 3/M)\n",
        "#cfg.model_name = \"ins1_mix_d10_l3_h3_t50K_s572091\"  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss 6.3e-07  (6/M fails e.g. 5068283822+4931712829=+09999996651 ModelAnswer: +19099996651) (7/M fails e.g. 3761900218-0761808615=+03000091603 ModelAnswer: +02000091603)\n",
        "#cfg.model_name = \"ins1_mix_d10_l3_h3_t50K_gf_s572091\" # Add/SubAccuracy=Four9s/Four9s. GrokFast.AvgFinalLoss 7.7e-07 (14/M fails e.g. 5852456231+2157444763=+08009900994 ModelAnswer: +08019900994) (23/M fails e.g. 4270534790-1971560790=+02298974000 ModelAnswer: +02298973000)\n",
        "\n",
        "# Mixed model initialized with addition model. Reset useful heads every 100 epochs.\n",
        "#cfg.model_name = \"ins2_mix_d6_l4_h4_t40K_s372001\"  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=1.7e-08. (3/M fails e.g. 530757+460849=+0991606 ModelAnswer: +0091606) (8 fails e.g. 261926-161857=+0100069 ModelAnswer: +0000069)\n",
        "\n",
        "# Mixed model initialized with addition model. Reset useful heads & MLPs every 100 epochs.\n",
        "#cfg.model_name = \"ins3_mix_d6_l4_h3_t40K_s372001\"  # Add/SubAccuracy=Four9s/Two9s. AvgFinalLoss=3.0e-04. (17/M fails e.g. 273257+056745=+0330002 ModelAnswer: +0320002) (3120 fails e,g. 09075-212133=-0003058 ModelAnswer: +0003058)\n",
        "\n",
        "# Mixed models initialized with addition model.\n",
        "#cfg.model_name = \"ins4_mix_d6_l3_h4_t30K_s775824\"  # Add/SubAccuracy=???/???\n",
        "#cfg.model_name = \"ins4_mix_d6_l2_h4_t30K_s775824\"  # Add/SubAccuracy=???/???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_IIpX2H2tNe"
      },
      "source": [
        "# Part 1B: Configuration: Input and Output file names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5BiELcf53ms"
      },
      "outputs": [],
      "source": [
        "cfg.reset_useful()\n",
        "cfg.reset_algo()\n",
        "\n",
        "main_fname = cfg.file_config_prefix\n",
        "if cfg.model_name != \"\":\n",
        "  main_fname = cfg.model_name\n",
        "\n",
        "  # Update cfg member data n_digits, n_layers, n_heads, n_training_steps from model_name\n",
        "  cfg.parse_model_name()\n",
        "\n",
        "  cfg.batch_size = 512 # Default analysis batch size\n",
        "  if cfg.n_layers >= 3 and cfg.n_heads >= 4:\n",
        "    cfg.batch_size = 256 # Reduce batch size to avoid memory constraint issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0DJkn5l2gq3"
      },
      "outputs": [],
      "source": [
        "main_fname_pth = main_fname + '.pth'\n",
        "main_fname_train_json = main_fname + '_train.json'\n",
        "main_fname_behavior_json = main_fname + '_behavior.json'\n",
        "main_fname_maths_json = main_fname + '_maths.json'\n",
        "main_repo_name=\"PhilipQuirke/VerifiedArithmetic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfdCBTuqh4zq"
      },
      "outputs": [],
      "source": [
        "# Update \"cfg\" with additional training config information from say\n",
        "#      https://huggingface.co/PhilipQuirke/VerifiedArithmetic/raw/main/ins1_mix_d6_l3_h4_t40K_s372001_train.json\"\n",
        "training_data_json = qt.download_huggingface_json(main_repo_name, main_fname_train_json)\n",
        "training_loss_list = qt.load_training_json(cfg, training_data_json)\n",
        "print('Loaded main model training config / loss from', main_repo_name, main_fname_train_json)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_config():\n",
        "  print(\"%Add=\", cfg.perc_add, \"%Sub=\", cfg.perc_sub, \"%Mult=\", cfg.perc_mult, \"InsertMode=\", cfg.insert_mode, \"File=\", main_fname)"
      ],
      "metadata": {
        "id": "lI4vY-vSxvxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_config()\n",
        "print(\"weight_decay=\", cfg.weight_decay, \"lr=\", cfg.lr, \"batch_size=\", cfg.batch_size)\n",
        "print('Main model will be read from HuggingLab file', main_repo_name, main_fname_pth)\n",
        "print('Main model training config / loss will be read from HuggingLab file', main_fname_train_json)\n",
        "print('Main model behavior analysis tags will be saved to Colab temporary file', main_fname_behavior_json)\n",
        "print('Main model maths analysis tags will be saved to Colab temporary file', main_fname_maths_json)"
      ],
      "metadata": {
        "id": "f48_G6wr730Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Results: Model training loss graphs"
      ],
      "metadata": {
        "id": "keTceF_3zbVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.graph_file_suffix = \"pdf\" # Can be pdf, svg or png"
      ],
      "metadata": {
        "id": "SzwhtOfzuMYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_config()\n",
        "print( \"Avg loss over last 5 epochs\", cfg.avg_final_loss)\n",
        "print( \"Final epoch loss\", cfg.final_loss)"
      ],
      "metadata": {
        "id": "GCwcStUps62W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the model final training loss and graph loss over epochs\n",
        "if training_loss_list:\n",
        "  answer_digits = cfg.n_digits + 1\n",
        "  title_font_size=32\n",
        "  tick_font_size=24\n",
        "\n",
        "  qt.plot_loss_lines(cfg, 1500, [training_loss_list[:1500]], labels = ['All'], log_y=False,\n",
        "                       title='Training Loss', title_font_size=title_font_size, tick_font_size=tick_font_size)\n",
        "\n",
        "  full_title, fig = qt.plot_loss_lines(cfg, cfg.n_training_steps, [training_loss_list], labels = ['All'], log_y=True,\n",
        "                       title='Training Loss', title_font_size=title_font_size, tick_font_size=tick_font_size)\n",
        "  pio.write_image(fig, cfg.model_name + '_LogTrainingLoss.' + cfg.graph_file_suffix)"
      ],
      "metadata": {
        "id": "KYV9FWt3zfFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RfHXneJw6n"
      },
      "source": [
        "# Part 3A: Set Up: Vocabulary / Embedding / Unembedding\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeObQk2kzAv7"
      },
      "outputs": [],
      "source": [
        "cfg.initialize_maths_token_positions()\n",
        "qt.set_maths_vocabulary(cfg)\n",
        "qt.set_maths_question_meanings(cfg)\n",
        "print(cfg.token_position_meanings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6rUaYvjOcE"
      },
      "source": [
        "# Part 3B: Set Up: Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA16Nb2PJ7MB"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.training_seed,\n",
        ")\n",
        "\n",
        "cfg.main_model = HookedTransformer(ht_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiJhch4KCej"
      },
      "source": [
        "# Part 4: Set Up: Loss Function & Data Generator\n",
        "This maths loss function and data generator are imported from QuantaTools as logits_to_tokens_loss, loss_fn, maths_data_generator_core and maths_data_generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqzljhQ4KJU5"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" maths \"questions\" data generator function. Invoked using next().\n",
        "ds = qt.maths_data_generator( cfg )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtmioT1THbJA"
      },
      "outputs": [],
      "source": [
        "# Generate sample data generator (unit test)\n",
        "print(next(ds)[:3,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KJhCxFtNKfm"
      },
      "source": [
        "# Part 5: Set Up: Load Model from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMkB_8GNRc0"
      },
      "outputs": [],
      "source": [
        "print(\"Loading model from HuggingFace\", main_repo_name, main_fname_pth)\n",
        "\n",
        "cfg.main_model.load_state_dict(utils.download_file_from_hf(repo_name=main_repo_name, file_name=main_fname_pth, force_is_torch=True))\n",
        "cfg.main_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_zUfwrXl6P"
      },
      "source": [
        "# Part 7A: Set Up: Create sample maths questions\n",
        "\n",
        "Create a batch of manually-curated mathematics test questions, and cache some sample model prediction outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6Kt5XD-XU9t"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"ablation intervention\" configuration class\n",
        "acfg = qt.acfg\n",
        "acfg.reset_ablate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiK-0P44gvfS"
      },
      "outputs": [],
      "source": [
        "varied_questions = qt.make_maths_test_questions_and_answers(cfg)\n",
        "num_varied_questions = varied_questions.shape[0]\n",
        "\n",
        "qt.a_set_ablate_hooks(cfg) # Updates acfg\n",
        "qt.a_calc_mean_values(cfg, varied_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5clJnASWCPVN"
      },
      "outputs": [],
      "source": [
        "print(\"Num questions:\", num_varied_questions, \"Question length:\", len(varied_questions[0]))\n",
        "print(\"Sample Question:\", varied_questions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FwJW0tv4Nf"
      },
      "source": [
        "# Part 7B: Results: Can the model correctly predict sample questions?\n",
        "\n",
        "Ask the model to predict the varied_questions (without intervention) to see if the model gets them all right. Categorize answers by complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56siA_QKe0W"
      },
      "outputs": [],
      "source": [
        "# Test maths question prediction accuracy on the sample questions provided.\n",
        "# Does NOT use UsefulInfo.* information\n",
        "# Used to estimate the accuracy of the model's predictions.\n",
        "# Returns a reduced set of questions - removing questions that the model failed to answer.\n",
        "print_config()\n",
        "\n",
        "acfg.show_test_failures = True\n",
        "varied_questions = qt.test_maths_questions_by_complexity(cfg, acfg, varied_questions)\n",
        "acfg.show_test_failures = False\n",
        "\n",
        "num_varied_questions = varied_questions.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PjaQvhhayUL"
      },
      "source": [
        "# Part 9 : Results: Is the model 99.9999% accurate?\n",
        "\n",
        "The model's accuracy is 99.9999% (aka \"six 9s\") if it can predict one million questions with 0 or 1 failed predictions. If it has 2 to 10 failed predictions the model's accuracy is called 99.999% (aka \"five 9s\").\n",
        "\n",
        "Note: There may be very rare edge cases (say 1 in ten million) that did not appear in the test questions. So this empirical test can **not** prove 100% accuracy.\n",
        "\n",
        "If the model fails some questions, consider:\n",
        "- Adding a few of the failures into the \"test questions\" into qt.make_maths_test_questions_and_answers()\n",
        "- Understand the \"use case(s)\" driving these failures\n",
        "- Alter qt.maths_data_generator_core to enrich the training data with examples if these use case(s)\n",
        "- Retrain the model using the VerifiedArithmeticTrain Colab.  \n",
        "\n",
        "Takes ~25 mins to run for ins_mix_d6_l3_h4_t40K_s372001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_1m_test = False"
      ],
      "metadata": {
        "id": "OeB86OU_8X9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enriching data means adding more \"hard\" and subtractions questions. Enriched data was used during training. Using enrich_data does not much impact the  model accuracy measured here.  "
      ],
      "metadata": {
        "id": "CFaarlLVAMTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enrich_data = True"
      ],
      "metadata": {
        "id": "4Bz-kDiwAKNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsauYqjaxok"
      },
      "outputs": [],
      "source": [
        "if run_1m_test:\n",
        "    acfg.show_test_failures = True\n",
        "    qt.test_correctness_on_num_questions(cfg, acfg, num_questions=1000000, enrich_data=enrich_data)\n",
        "    acfg.show_test_failures = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXBYdxj-jLZc"
      },
      "source": [
        "# Part 10: Set Up: Which token positions are used by the model?\n",
        "\n",
        "Ablate all nodes in each (question and answer) token position (by overriding the model memory aka residual stream). If the model's prediction loss increases, the token position is useful to the algorithm. Unused token positions are excluded from further analysis. Used to populate the UsefulInfo.useful_positions data. This is token **position level** information.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGL57MRBLdUh"
      },
      "outputs": [],
      "source": [
        "num_failures_list = []\n",
        "acfg.show_test_failures = False\n",
        "\n",
        "for position in range(cfg.n_ctx):\n",
        "  # Test accuracy of model in predicting question answers. Ablates all nodes at acfg.ablate_position. Does NOT use UsefulInfo.* information.\n",
        "  num_fails = qt.test_maths_questions_by_impact(cfg, acfg, varied_questions, position, ablate=True)\n",
        "\n",
        "  if num_fails > 0:\n",
        "    # Add position to UsefulInfo.useful_positions\n",
        "    cfg.add_useful_position(position)\n",
        "    num_failures_list += [num_fails]\n",
        "  else:\n",
        "    num_failures_list += \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsGWUbILYin"
      },
      "source": [
        "# Part 11: Results: Which token positions are used by the model?\n",
        "\n",
        "Which token positions are is used in the model's predictions? Unused token positions are excluded from further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpRp5YMmLe1y"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print(\"num_questions=\", num_varied_questions)\n",
        "print(\"useful_positions=\", cfg.useful_positions )\n",
        "print()\n",
        "\n",
        "cfg.calc_position_failures_map(num_failures_list)\n",
        "qt.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904WBkTOLg_5"
      },
      "source": [
        "# Part 12A: Set Up: Which nodes are used by the model?\n",
        "\n",
        "Here we ablate each (attention head and MLP neuron) node in each (question and answer) token position see if the model's prediction loss increases. If loss increases then the \"node + token position\" is used by the algorithm. Used to calculate the UsefulInfo.useful_node_location. This is **position+node level** information. Unused nodes are excluded from further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9ot9RMMTAi"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes = qt.UsefulNodeList()\n",
        "\n",
        "qt.ablate_mlp_and_add_useful_node_tags(cfg, varied_questions, qt.test_maths_questions_and_add_useful_node_tags)\n",
        "qt.ablate_head_and_add_useful_node_tags(cfg, varied_questions, qt.test_maths_questions_and_add_useful_node_tags)\n",
        "qt.add_node_attention_tags(cfg, varied_questions)\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw-Wteh-JBd6"
      },
      "source": [
        "# Part 12B: Results: Which nodes are used by the model?\n",
        "\n",
        "Here are the (attention head and MLP neuron) node in each (question and answer) token position used by the model during predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhEWUt7yJBuh"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmQHiLALp-3"
      },
      "source": [
        " # Part 13: Set up: Show and save Quanta map\n",
        "\n",
        " Using the UsefulNodes and filtering their tags, show a 2D map of the nodes and the tag minor versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emcB0_CpYTaJ"
      },
      "outputs": [],
      "source": [
        "def show_quanta_map( title, major_tag : qt.QType, minor_tag : str, get_node_details,\n",
        "        image_width_inches : int = -1, image_height_inches : int = -1,\n",
        "        blue_shades : bool = True, cell_num_shades : int = 6,\n",
        "        filters : qt.FilterNode = None, cell_fontsize : int = 9,\n",
        "        combine_identical_cells : bool = True, show_perc_circles : bool = False ):\n",
        "\n",
        "  test_nodes = cfg.useful_nodes\n",
        "  if filters is not None:\n",
        "    test_nodes = qt.filter_nodes(test_nodes, filters)\n",
        "\n",
        "  ax1, quanta_results, num_results = qt.calc_quanta_map(\n",
        "      cfg, blue_shades, cell_num_shades,\n",
        "      test_nodes, major_tag.value, minor_tag, get_node_details,\n",
        "      cell_fontsize, combine_identical_cells, show_perc_circles,\n",
        "      image_width_inches, image_height_inches )\n",
        "\n",
        "  if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "      print(\"Saving quanta map:\", title)\n",
        "      qt.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "      ax1.set_title(cfg.file_config_prefix() + ' ' + title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpkyhHRoMOSw"
      },
      "source": [
        "# Part 16A: Results: Show failure percentage map\n",
        "\n",
        "Show the percentage failure rate (incorrect prediction) when individual Attention Heads and MLPs are ablated. Lower percentages correspond to rarer edge cases. The grey space represents nodes that are not used by the model.\n",
        "\n",
        "A cell containing \"< 1\" may add some risk to the accuracy of the overall analysis process. Check to see if this represents a new use case. Improve the test data set to contain more instances of this (new or existing) use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ28dx5YLs0P"
      },
      "outputs": [],
      "source": [
        "show_quanta_map( \"Failure Frequency Behavior Per Node\", qt.QType.FAIL, \"\", qt.get_quanta_fail_perc,\n",
        "                cell_num_shades = qt.FAIL_SHADES, combine_identical_cells = False, show_perc_circles = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifmtnLoTCN5"
      },
      "source": [
        "# Part 16B - Show answer impact behavior map\n",
        "\n",
        "This map shows the answer digit(s) A0 .. An+1 impacted when we ablate each useful node in the  model. Cells containing values like A5..2 are used in multiple prediction steps to calculate multiple answer digits e.g. A2 to A5. Late token\n",
        "positions focus on predicting one answer digit - partially by using results calculated in early token positions.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K9PyCKYTUzX"
      },
      "outputs": [],
      "source": [
        "show_quanta_map( \"Answer Impact Behavior Per Node\", qt.QType.IMPACT, \"\", qt.get_quanta_impact,\n",
        "                cell_num_shades = cfg.num_answer_positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCfaCT1Puhz"
      },
      "source": [
        "# Part 16C: Result: Show attention map\n",
        "\n",
        "This map shows the input tokens each useful attention head attends to at each token position.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTGoEWHgvFH6"
      },
      "outputs": [],
      "source": [
        "# Only maps attention heads, not MLP layers\n",
        "show_quanta_map( \"Attention Behavior Per Head\", qt.QType.ATTN, \"\", qt.get_quanta_attention,\n",
        "                # image_height_inches = 8, # image_width_inches = 11,\n",
        "                cell_num_shades = qt.ATTN_SHADES )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7-99ZxDOrbF"
      },
      "source": [
        "# Part 16C - Show question complexity map\n",
        "\n",
        "This map shows whether each useful node is used\n",
        "to answer the quesstion classes: addition (S), positive-answer subtraction (M) and/or negative-answer subtraction (N) questions. In mixed models, nodes may be used in prediction of two or three questions classes. That is they are polysemantic.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKeybAj3d6QU"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  num_add, num_sub, num_neg, num_triple, num_double, num_single = qt.get_maths_nodes_operation_coverage(cfg.useful_nodes.nodes)\n",
        "  print( \"# useful nodes:\", len(cfg.useful_nodes.nodes))\n",
        "  print( \"# useful nodes involved in S, M, N operations:\", num_add, num_sub, num_neg )\n",
        "  print( \"# useful nodes involved in 3, 2, 1 operations:\", num_triple, num_double, num_single)\n",
        "  print()\n",
        "\n",
        "  # For each useful cell, show if addition (S), positive-answer subtraction (M) and negative-answer subtraction (N) questions relies on the node.\n",
        "  show_quanta_map( \"Maths Operation Coverage\", qt.QType.MATH, \"\", qt.get_maths_operation_complexity,\n",
        "                  blue_shades = False, cell_num_shades = 4, combine_identical_cells = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxsi3TN7S0S"
      },
      "source": [
        "This map shows the simpliest (lowest complexity) addition quanta S0, S1, etc impacted when we ablate each node in an addition or mixed model. To answer S0 questions, only the S0 nodes are used. To answer S1 questions, S0 and S1 nodes are used, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyoErRoCA-pz"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_add > 0:\n",
        "  # For each useful cell, show the minimum addition question complexity that relies on the node, as measured using quanta S0, S1, S2, ...\n",
        "  show_quanta_map( \"Addition Min-Complexity\", qt.QType.MATH_ADD, qt.MathsBehavior.ADD_COMPLEXITY_PREFIX.value, qt.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qt.MATH_ADD_SHADES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4DTadZo7e9a"
      },
      "source": [
        "This map shows the simpliest (lowest complexity) subtraction quanta M0, M1, etc impacted when we ablate each node in an subtraction or mixed model. To answer M0 questions, only the M0 nodes are used. To answer M1 questions, M0 and M1 nodes are used, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZzbjxUBQGE"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  # For each useful cell, show the minimum \"positive-answer subtraction\" question complexity that relies on the node, as measured using quanta M0, M1, M2, ...\n",
        "  show_quanta_map( \"Positive-answer Subtraction Min-Complexity\", qt.QType.MATH_SUB, qt.MathsBehavior.SUB_COMPLEXITY_PREFIX.value, qt.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qt.MATH_SUB_SHADES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUaT47ettc0M"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  # For each useful cell, show the minimum \"negative-answer subtraction\" question complexity that relies on the node, as measured using quanta N0, N1, N2, ...\n",
        "  show_quanta_map( \"Negative-answer Subtraction Min-Complexity\", qt.QType.MATH_NEG, qt.MathsBehavior.NEG_COMPLEXITY_PREFIX.value, qt.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qt.MATH_SUB_SHADES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbiau9foMp3h"
      },
      "source": [
        "# Part 19A: Results: Manual interpretation of PCA results\n",
        "\n",
        "Principal Component Analysis (PCA) is a powerful technique that aids in mechanistic interpretability by simplifying complex datasets into principal components that capture the most significant variance within the data.\n",
        "\n",
        "This library uses PCA to help understand the purpose of individual useful nodes. For more background refer https://github.com/PhilipQuirke/verified_transformers/blob/main/pca.md\n",
        "\n",
        "If an attention head and an answer digit An gives an interpretable response (2 or 3 distinct output clusters) on 3 groups of questions aligned to ST8, ST9 and ST10 definitions, then plot the response and add a PCA tag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlgEAPV1g7W"
      },
      "outputs": [],
      "source": [
        "# Create a cache of sample maths questions based on the T8, T9, T10 categorisation in cfg.tricase_questions_dict\n",
        "qt.make_maths_tricase_questions(cfg)\n",
        "\n",
        "cfg.useful_nodes.reset_node_tags(qt.QType.MATH_ADD.value, qt.MathsBehavior.ADD_PCA_TAG.value)\n",
        "cfg.useful_nodes.reset_node_tags(qt.QType.MATH_SUB.value, qt.MathsBehavior.SUB_PCA_TAG.value)\n",
        "cfg.useful_nodes.reset_node_tags(qt.QType.MATH_NEG.value, qt.MathsBehavior.NEG_PCA_TAG.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ5fS3XNMs8e"
      },
      "outputs": [],
      "source": [
        "# Plot all attention heads with the clearest An selected. Data is manually selected\n",
        "if pca_lib_avail:\n",
        "\n",
        "  if cfg.model_name == \"add_d5_l1_h3_t15K_s372001\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[ 12, 0, 0, 4 ],\n",
        "      [ 12, 0, 2, 3 ],\n",
        "      [ 13, 0, 0, 3 ],\n",
        "      [ 14, 0, 0, 2 ],\n",
        "      [ 15, 0, 0, 1 ]])\n",
        "\n",
        "  elif cfg.model_name == \"add_d5_l2_h3_t15K_s372001\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[10, 0, 0, 2 ],\n",
        "      [ 10, 0, 2, 1 ],\n",
        "      [ 12, 0, 0, 3 ],\n",
        "      [ 12, 1, 0, 3 ],\n",
        "      [ 12, 1, 1, 4 ],\n",
        "      [ 12, 1, 2, 4 ],\n",
        "      [ 13, 0, 0, 3 ],\n",
        "      [ 13, 1, 2, 2 ],\n",
        "      [ 14, 0, 0, 2 ],\n",
        "      [ 14, 1, 2, 2 ],\n",
        "      [ 15, 0, 0, 1 ],\n",
        "      [ 15, 1, 1, 1 ]])\n",
        "\n",
        "  elif cfg.model_name == \"add_d6_l2_h3_t15K_s372001\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[11, 0, 1, 1 ], # EVR[0]=31% but needed for D1.ST\n",
        "      [ 11, 0, 0, 2 ],\n",
        "      [ 12, 0, 0, 3 ],\n",
        "      [ 13, 0, 0, 1 ],\n",
        "      [ 13, 0, 1, 0 ], # EVR[0]=32% but needed for D0.ST\n",
        "      [ 14, 0, 0, 4 ],\n",
        "      [ 14, 1, 1, 4 ],\n",
        "      [ 15, 0, 0, 4 ],\n",
        "      [ 15, 1, 1, 4 ],\n",
        "      [ 15, 1, 2, 4 ],\n",
        "      [ 16, 0, 0, 3 ],\n",
        "      [ 16, 1, 1, 3 ],\n",
        "      [ 17, 0, 0, 2 ],\n",
        "      [ 17, 1, 1, 2 ],\n",
        "      [ 18, 0, 0, 1 ]])\n",
        "\n",
        "\n",
        "  elif cfg.model_name == \"add_d6_l2_h3_t20K_s173289\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[ 14, 1, 0, 5 ],\n",
        "      [ 14, 1, 1, 4 ],\n",
        "      [ 14, 1, 2, 4 ],\n",
        "      [ 15, 1, 1, 4 ],\n",
        "      [ 15, 1, 2, 4 ],\n",
        "      [ 16, 1, 1, 3 ],\n",
        "      [ 16, 1, 2, 3 ],\n",
        "      [ 17, 1, 1, 2 ],\n",
        "      [ 18, 1, 1, 1 ]])\n",
        "\n",
        "  elif cfg.model_name == \"add_d6_l2_h3_t20K_s572091\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[ 10, 0, 0, 3 ],\n",
        "      [ 11, 0, 0, 2 ],\n",
        "      [ 12, 0, 0, 1 ],\n",
        "      [ 14, 0, 0, 4 ],\n",
        "      [ 14, 1, 0, 4 ],\n",
        "      [ 14, 1, 1, 4 ],\n",
        "      [ 14, 1, 2, 3 ],\n",
        "      [ 15, 0, 0, 4 ],\n",
        "      [ 15, 1, 0, 4 ],\n",
        "      [ 15, 1, 1, 4 ],\n",
        "      [ 15, 1, 2, 4 ],\n",
        "      [ 16, 0, 0, 3 ],\n",
        "      [ 16, 1, 0, 3 ],\n",
        "      [ 16, 1, 1, 3 ],\n",
        "      [ 17, 0, 0, 2 ],\n",
        "      [ 17, 1, 1, 2 ],\n",
        "      [ 18, 0, 0, 1 ]])\n",
        "\n",
        "  elif cfg.model_name == \"add_d10_l2_h3_t40K_s572091\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[ 20, 0, 0, 1 ],\n",
        "      [ 23, 1, 0, 8 ],\n",
        "      [ 24, 1, 0, 7 ],\n",
        "      [ 22, 1, 0, 8 ],\n",
        "      [ 25, 1, 0, 6 ],\n",
        "      [ 26, 1, 0, 5 ],\n",
        "      [ 27, 1, 0, 4 ],\n",
        "      [ 27, 1, 2, 4 ],\n",
        "      [ 28, 1, 0, 3 ],\n",
        "      [ 29, 1, 0, 2 ],\n",
        "      [ 30, 1, 0, 1 ]])\n",
        "\n",
        "  elif cfg.model_name.startswith(\"sub_d6_l2_h3_t30K\"):\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.MINUS,\n",
        "      [[ 9, 0, 1, 3 ],\n",
        "      [ 10, 0, 1, 2 ],\n",
        "      [ 11, 0, 1, 1 ],\n",
        "      [ 13, 0, 1, 4 ],\n",
        "      [ 13, 1, 2, 5 ],\n",
        "      [ 15, 0, 0, 5 ],\n",
        "      [ 15, 1, 1, 0 ],\n",
        "      [ 15, 1, 1, 1 ],\n",
        "      [ 15, 1, 1, 2 ],\n",
        "      [ 15, 1, 1, 3 ],\n",
        "      [ 15, 1, 1, 4 ],\n",
        "      [ 15, 1, 2, 0 ],\n",
        "      [ 15, 1, 2, 1 ],\n",
        "      [ 15, 1, 2, 2 ],\n",
        "      [ 15, 1, 2, 3 ],\n",
        "      [ 16, 0, 0, 0 ],\n",
        "      [ 16, 0, 0, 1 ],\n",
        "      [ 16, 0, 0, 2 ],\n",
        "      [ 16, 0, 0, 3 ],\n",
        "      [ 16, 0, 0, 4 ],\n",
        "      [ 16, 0, 0, 5 ],\n",
        "      [ 16, 1, 0, 4 ],\n",
        "      [ 16, 1, 1, 0 ],\n",
        "      [ 16, 1, 1, 1 ],\n",
        "      [ 16, 1, 1, 2 ],\n",
        "      [ 16, 1, 1, 3 ],\n",
        "      [ 16, 1, 1, 4 ],\n",
        "      [ 16, 1, 2, 0 ],\n",
        "      [ 16, 1, 2, 1 ],\n",
        "      [ 16, 1, 2, 2 ],\n",
        "      [ 16, 1, 2, 3 ],\n",
        "      [ 16, 1, 2, 4 ],\n",
        "      [ 16, 1, 2, 5 ],\n",
        "      [ 17, 0, 0, 0 ],\n",
        "      [ 17, 0, 0, 1 ],\n",
        "      [ 17, 0, 0, 2 ],\n",
        "      [ 17, 0, 0, 3 ],\n",
        "      [ 17, 1, 0, 3 ],\n",
        "      [ 17, 1, 0, 4 ],\n",
        "      [ 17, 1, 2, 4 ],\n",
        "      [ 18, 0, 0, 0 ],\n",
        "      [ 18, 0, 0, 1 ],\n",
        "      [ 18, 0, 0, 2 ],\n",
        "      [ 18, 1, 0, 2 ],\n",
        "      [ 18, 1, 2, 3 ],\n",
        "      [ 19, 0, 0, 0 ],\n",
        "      [ 19, 1, 2, 2 ],\n",
        "      [ 20, 0, 0, 0 ]])\n",
        "\n",
        "  elif cfg.model_name == \"mix_d6_l3_h4_t40K_s372001\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[ 8, 1, 0, 4 ],\n",
        "      [ 11, 1, 0, 1 ],\n",
        "      [ 12, 1, 0, 0 ],\n",
        "      [ 13, 1, 1, 1 ],\n",
        "      [ 14, 2, 1, 5 ],\n",
        "      [ 15, 2, 1, 4 ],\n",
        "      [ 16, 2, 1, 3 ],\n",
        "      [ 17, 2, 1, 2 ],\n",
        "      [ 18, 1, 0, 4 ],\n",
        "      [ 18, 2, 1, 1 ]])\n",
        "\n",
        "  elif cfg.model_name == \"ins1_mix_d6_l3_h4_t40K_s372001\":\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "      [[10, 0, 0, 2 ],\n",
        "      [ 10, 0, 1, 2 ],\n",
        "      [ 11, 0, 0, 2 ],\n",
        "      [ 11, 0, 1, 1 ],\n",
        "      [ 13, 0, 1, 0 ],\n",
        "      [ 13, 1, 3, 1 ],\n",
        "      [ 14, 1, 2, 0 ],\n",
        "      [ 14, 1, 2, 2 ],\n",
        "      [ 14, 1, 3, 4 ],\n",
        "      [ 15, 0, 3, 5 ],\n",
        "      [ 15, 1, 2, 2 ],\n",
        "      [ 15, 1, 3, 4 ],\n",
        "      [ 16, 0, 3, 4 ],\n",
        "      [ 16, 1, 2, 0 ],\n",
        "      [ 16, 1, 2, 1 ],\n",
        "      [ 16, 1, 2, 2 ],\n",
        "      [ 16, 1, 3, 2 ],\n",
        "      [ 17, 0, 3, 3 ],\n",
        "      [ 17, 1, 2, 2 ],\n",
        "      [ 17, 1, 3, 2 ],\n",
        "      [ 18, 0, 3, 2 ],\n",
        "      [ 18, 1, 3, 1 ],\n",
        "      [ 19, 0, 3, 1 ],\n",
        "      [ 19, 2, 0, 0 ],\n",
        "      [ 19, 2, 1, 0 ],\n",
        "      [ 20, 0, 0, 0 ],\n",
        "      [ 20, 0, 3, 0 ]])\n",
        "\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.MINUS,\n",
        "      [[10, 0, 0, 2 ],\n",
        "      [ 10, 0, 1, 2 ],\n",
        "      [ 11, 0, 0, 2 ],\n",
        "      [ 11, 0, 1, 1 ],\n",
        "      [ 13, 0, 1, 0 ],\n",
        "      [ 14, 0, 0, 4 ],\n",
        "      [ 14, 0, 2, 5 ],\n",
        "      [ 14, 1, 2, 0 ],\n",
        "      [ 14, 1, 2, 2 ],\n",
        "      [ 14, 1, 3, 4 ]])\n",
        "\n",
        "  print('Finished generating plots')\n",
        "\n",
        "else:\n",
        "  print( \"PCA library failed to import. So PCA not done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu3Pr9CMx3l"
      },
      "source": [
        "# Part 19B: Results: Automatic interpretation of PCA results (Optional)\n",
        "\n",
        "Part 19B is manual and selective. This part is automatic. It tests nodes not included in Part 19A, where this first (single) principal component explains 75% or more of the node. It adds a QType.PCA \"weak\" tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-8LlNMiqxyW"
      },
      "outputs": [],
      "source": [
        "do_auto_pca = False # Suppress for speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UADsW9c1g7X"
      },
      "outputs": [],
      "source": [
        "def auto_node_pca(ax, index, node_location, operation, answer_digit, evr_perc_threshold):\n",
        "\n",
        "    title, error_message = qt.maths_tools._build_title_and_error_message(\n",
        "        cfg=cfg, node_location=node_location, operation=operation, answer_digit=answer_digit\n",
        "    )\n",
        "\n",
        "    if (answer_digit, operation) in cfg.tricase_questions_dict:\n",
        "        test_inputs = cfg.tricase_questions_dict[(answer_digit, operation)]\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    pca, pca_attn_outputs, title = qt.calc_pca_for_an(\n",
        "        cfg=cfg, node_location=node_location, title=title, error_message=error_message, test_inputs=test_inputs\n",
        "    )\n",
        "\n",
        "    if pca is not None:\n",
        "      perc = qt.pca_evr_0_percent(pca)\n",
        "      if perc > evr_perc_threshold:\n",
        "        qt.maths_tools.plot_pca_for_an(ax, pca_attn_outputs, title)\n",
        "\n",
        "        major_tag = qt.QType.MATH_ADD if operation == qt.MathsToken.PLUS else qt.QType.MATH_SUB # Does not handle NEG case\n",
        "        cfg.add_useful_node_tag( node_location, major_tag.value, qt.maths_tools.pca_op_tag(answer_digit, operation, False) )\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDZ4--e11g7X"
      },
      "outputs": [],
      "source": [
        "def auto_find_pca_node(node, op, evr_perc_threshold):\n",
        "  # Allow up to 12 graphs\n",
        "  num_rows = 3\n",
        "  num_cols = 4\n",
        "\n",
        "  fig, axs = plt.subplots(num_rows, num_cols)\n",
        "  fig.set_figheight(4)\n",
        "  fig.set_figwidth(10)\n",
        "\n",
        "  index = 0\n",
        "  for answer_digit in range(cfg.n_digits+1):\n",
        "    ax = axs[index // num_cols, index % num_cols]\n",
        "    if auto_node_pca(ax, index, node, op, answer_digit, evr_perc_threshold):\n",
        "      index += 1\n",
        "\n",
        "  # Remove any graphs we dont need after all\n",
        "  index2 = index\n",
        "  while index2 < num_rows * num_cols:\n",
        "    ax = axs[index2 // num_cols, index2 % num_cols]\n",
        "    ax.remove()\n",
        "    index2 += 1\n",
        "\n",
        "  if index > 0:\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCYAud_Y1g7X"
      },
      "outputs": [],
      "source": [
        "def auto_find_pca(operation):\n",
        "  print(\"Automatic (weak) PCA tags for\", cfg.model_name, \"with operation\", qt.token_to_char(cfg, operation))\n",
        "  evr_perc_threshold = 75 # Generally use 75%. Have seen useful cells as low as 33%\n",
        "\n",
        "  for node in cfg.useful_nodes.nodes:\n",
        "\n",
        "    # Exclude nodes with a (manual) PCA tag - for any answer digit(s)). Exclude MLP neurons.\n",
        "    major_tag = qt.QType.MATH_ADD if operation == qt.MathsToken.PLUS else qt.QType.MATH_SUB # Does not handle NEG case\n",
        "    minor_tag_prefix = qt.MathsBehavior.ADD_PCA_TAG if operation == qt.MathsToken.PLUS else qt.MathsBehavior.SUB_PCA_TAG\n",
        "    if node.is_head and not node.contains_tag(major_tag.value, minor_tag_prefix.value):\n",
        "      print( \"Doing PCA on node\", node.name())\n",
        "\n",
        "      auto_find_pca_node(node, operation, evr_perc_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2Mwk8Ip-pB"
      },
      "outputs": [],
      "source": [
        "if do_auto_pca:\n",
        "  if pca_lib_avail:\n",
        "    if cfg.perc_add > 0:\n",
        "      auto_find_pca(qt.MathsToken.PLUS)\n",
        "    if cfg.perc_sub > 0:\n",
        "      auto_find_pca(qt.MathsToken.MINUS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFcCpfmKwlAH"
      },
      "source": [
        "# Part 20: Results: Show useful nodes and behaviour tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzIaqmtwmkH"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.sort_nodes()\n",
        "cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVkOJRmPvPms"
      },
      "source": [
        "# Part 21: Results: Save useful nodes and behaviour tags to json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naD2eCZevOsi"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
        "print( \"Saving useful node list with behavior tags:\", main_fname_behavior_json)\n",
        "cfg.useful_nodes.save_nodes(main_fname_behavior_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXc9bD2q0zC"
      },
      "source": [
        "# Part 22 : Results: Search for model algorithm tasks\n",
        "\n",
        "Here we find which model nodes perform which specific algorithm task.\n",
        "- **Automatic searches** for node purposes are preferred, as they applicable to several models, and survive (non-significant, node-reordering) differences between models caused by differences in training.\n",
        "- **Manually written tests** of node purposes, specific to a single model instance are also supported.\n",
        "\n",
        "The qt.search_and_tag searches for a task on useful nodes by:\n",
        "- **filtering** useful nodes, based on \"tag\" pre-requisites, to find the few nodes worth doing investigating. For more detail refer https://github.com/PhilipQuirke/verified_transformers/blob/main/filter.md\n",
        "- **intervention ablation** testing on the interesting nodes:\n",
        "  - The first \"store\" question is run without hooks\n",
        "  - The second \"clean\" question is run with hooks interjecting some data from the \"store\" run. This run gives, not a \"clean\" answer, but an \"intervened\" answer, which mixes the \"store\" answer and the \"clean\" answer. Our beliefs about the nodes algorthmic purpose are baked into the store question, clean question and intervened answer.\n",
        "- An **algorithm tag** is added to all interesting nodes that pass the intervention ablation test(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyVYckFtV-RV"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.reset_node_tags(qt.QType.ALGO.value)\n",
        "acfg.show_test_failures = False\n",
        "acfg.show_test_successes = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgg43Sqe8oF1"
      },
      "source": [
        "## Part 22B: Automated An.SS search\n",
        "\n",
        " Search for addition \"Use Sum 9\" (SS) tasks e.g. 34633+55555=+090188 where D4 and D'4 sum to 9 (4+5), and D3 + D'3 > 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUYf20mk8zls"
      },
      "outputs": [],
      "source": [
        "qt.search_and_tag( cfg, acfg, qt.add_ss_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5DMV3I_25ST"
      },
      "source": [
        "## Part 22C: Automated An.SC search\n",
        "\n",
        "Search for addition \"Make Carry 1\" (SC) tasks e.g. 222222+666966=+0889188 where D2 + D'2 > 10.\n",
        "\n",
        "(Sometimes model chooses to use ST **instead** of SC. Sometimes model chooses to use ST **and** SC. For A1, model can **accurately** use just SC. For A0,  SC and ST are not needed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvS1eOReZUq1"
      },
      "outputs": [],
      "source": [
        "#acfg.show_test_failures = True\n",
        "qt.search_and_tag( cfg, acfg, qt.add_sc_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iosx5zE_macF"
      },
      "source": [
        "## Part 22D: Automated An.SA search\n",
        "\n",
        "Search for addition \"Simple Add\" (SA) tasks e.g. 555555+111111=+0666666 where D3 + D'3 < 10\n",
        "\n",
        "The SA tasks is sometimes split/shared over 2 attention heads in the same position and layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIktdaRL-BfP"
      },
      "outputs": [],
      "source": [
        "#acfg.show_test_failures = True\n",
        "qt.search_and_tag( cfg, acfg, qt.add_sa_functions,\n",
        "                  do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThzFD1FxBXg8"
      },
      "source": [
        "## Part 22E: Automated An.ST search\n",
        "\n",
        "Search for A0.ST to A5.ST with impact \"A65432\" to \"A65\" in early tokens.\n",
        "\n",
        "A0 and A1 are simple to calculate and so do NOT use An.ST or An.STm values. So A0 and A1 are excluded from the answer impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFJV4pfWSwHQ"
      },
      "outputs": [],
      "source": [
        "qt.search_and_tag( cfg, acfg, qt.add_st_functions,\n",
        "                  do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jOBIUDzRrGz"
      },
      "source": [
        "## Part 22F: Automated An.MD search\n",
        "\n",
        "Search for positive-answer subtraction \"Difference\" (MD) tasks e.g. 666666-222222=+0444444 where D3 >= D'3\n",
        "\n",
        "The MD task may be split/shared over 2 attention heads in the same position at the same layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKag6sUnVS2N"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qt.search_and_tag( cfg, acfg, qt.sub_md_functions,\n",
        "                    do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyItdhmhIn8"
      },
      "source": [
        "## Part 22G: Automated An.MB search\n",
        "\n",
        "Search for positive-answer subtraction \"Borrow One\" (MB) tasks e.g. 222222-111311=+0110911 where D2 > D'2\n",
        "\n",
        "(Sometimes model chooses to use MT **instead** of MB. Sometimes model chooses to use MT **and** MB. For A1, model can **accurately** use just MB. For A0,  MB and MT are not needed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzjlFwEui7K9"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qt.search_and_tag( cfg, acfg, qt.sub_mb_functions,\n",
        "                    allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJo37Qg2ZQpJ"
      },
      "source": [
        "## Part 22H: Automated An.MT search\n",
        "\n",
        "For accuracy, the addition algorithm calculates cascading \"carry one\" in early tokens using the An.ST sub-task. Paralleling this, the subtraction algorithm calculates cascading \"borrow one\" in early tokens using the An.MT sub-task.\n",
        "\n",
        "This section locates An.MT sub-tasks.\n",
        "\n",
        "Define An.MT = +1 if Dn > D'n else 0 if Dn == D'n else -1  \n",
        "The cascading \"borrow one\" calculation is then:\n",
        "A3.MV = fn(A3.MT, fn(A2.MT, fn(A1.MT, A0.MT)))\n",
        "where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "and the output \"-1\" means a cascading borrow one.\n",
        "\n",
        "The above fn could be simplified, but the (below) GT sub-task often relies on the above definition. The tricase An.MT definition also mirrors the addition An.ST definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOJ4J214g3La"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "    #acfg.show_test_failures = True\n",
        "    #acfg.show_test_successes = True\n",
        "    qt.search_and_tag( cfg, acfg, qt.sub_mt_functions,\n",
        "                      do_pair_search = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agw7np7gZmgP"
      },
      "source": [
        "## Part 22I: Automated OPR search\n",
        "\n",
        "For mixed models that do addition and subtraction the operation token \"+/-\" (in the middle of the question) is key. Find nodes that attend to the question operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0s2ZTMLimIR"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0 and cfg.perc_add > 0 :\n",
        "  qt.search_and_tag( cfg, acfg, qt.opr_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztfj5n8FjYNt"
      },
      "source": [
        "## Part 22J: Automated SGN search\n",
        "\n",
        "For mixed models that do addition and subtraction, and for our subtraction models, the answer sign token \"+/-\" (at the start of the answer) is important. Find nodes that attend to the answer sign token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8smeZ3vBkIQ3"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qt.search_and_tag( cfg, acfg, qt.sgn_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOVsZIpoQN_r"
      },
      "source": [
        "## Part 22K: Automated An.ND search\n",
        "\n",
        "Search for negative-answer subtraction Difference (ND) tasks e.g. 033333-111111=-077778 where D < D'\n",
        "\n",
        "The ND task may be split/shared over 2 attention heads in the same position at the same layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmBOMTeqRJXf"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qt.search_and_tag( cfg, acfg, qt.neg_nd_functions,\n",
        "                    do_pair_search = True, allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF4AjvfG2QDy"
      },
      "source": [
        "## Part 22L: Automated An.NB search\n",
        "\n",
        "Search for negative-answer subtraction Borrow One (NB) tasks e.g. 033333-111411=-078078 where D < D' and D2 < D'2\n",
        "\n",
        "(Sometimes model chooses to use NT **instead** of NB. Sometimes model chooses to use NT **and** NB.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvtV6ZM48O4"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qt.search_and_tag( cfg, acfg, qt.neg_nb_functions,\n",
        "                    allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdWYQKi1Sr3M"
      },
      "source": [
        "## Part 22M: Automated An.GT search\n",
        "\n",
        "Both SUB (e.g. 00600-00201=+000399) and NEG (00100-00201=-000101) questions rely on knowing whether D > D'. How is this calculated?\n",
        "\n",
        "Approach 1: Model has specific GT nodes:\n",
        "Define An.GT = +1 if Dn > D'n else 0 if Dn = D'n else -1  \n",
        "When n_digits = 4, D > D' = f(A3.GT, fn(A2.GT, fn(A1.GT, A0.GT)))\n",
        "Where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "\n",
        "Approach 2: Model leverages the existing MT nodes:\n",
        "When n_digits = 4, D > D' = f(A3.MT, fn(A2.MT, fn(A1.MT, A0.MT)))\n",
        "where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "\n",
        "Usually, one node performs both say A3.MT and A3.GT sub-tasks, but in some models the A3.MT and A3.GT functions are performed by distinct nodes. Hence we test for the MT and GT behavior separately.\n",
        "\n",
        "Both approaches mirrors the calculation style used in ADD to calculate Amax as 1 or 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5f6Y2N-Vrry"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "    #acfg.show_test_successes = False\n",
        "    qt.search_and_tag(cfg, acfg, qt.sub_gt_functions,\n",
        "                      allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQLfkdlbVAl_"
      },
      "source": [
        "# Part 23: Show algorithm quanta map\n",
        "\n",
        "This map shows a compacted view of all useful token positions (horizontally) and all useful attention heads and MLP layers\n",
        "(vertically) used in predictions as blue cells. In each cell, the algorithm sub-task(s) Base Add SA, Make Carry SC, TriCase ST, etc found by automated search with ablation testing are shown.\n",
        "\n",
        "Sometimes a subtask is logIcally shared across two attention heads. The SA, MD and ND subtasks sometimes do this.\n",
        "\n",
        "This map plots the \"algorithm\" tags generated in previous steps as a quanta map. This is an automatically generated partial explanation of the model algorithm.\n",
        "\n",
        "Nodes with multiple tags were tagged (found) by more than one of the above subtask searches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If a cell only has an OPR tag or only has a SGN tag then we do not understood its purpose.\n",
        "# The tag is just an \"attention\" fact. Remove these tags from the algorithm map\n",
        "# (A cell that has both OPR and SGN tags, we believe it is a \"Select question case\" node. We keep it)\n",
        "for node in cfg.useful_nodes.nodes:\n",
        "    tags = node.filter_tags(qt.QType.ALGO.value)\n",
        "    if len(tags) == 1:\n",
        "        only_tag = tags[0]\n",
        "        if qt.MathsTask.OPR_TAG.value in only_tag or qt.MathsTask.SGN_TAG.value in only_tag:\n",
        "            print( \"Removing\", node.name(), only_tag)\n",
        "            node.reset_tags(qt.QType.ALGO.value)"
      ],
      "metadata": {
        "id": "Q4bvm3yzQ8Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCHcurqILvtN"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "qt.print_algo_purpose_results(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRWSKaURK1L2"
      },
      "outputs": [],
      "source": [
        "# Show useful nodes that have identified algorithm sub-task tags\n",
        "show_quanta_map( \"Maths Purpose Per Node\", qt.QType.ALGO, \"\", qt.get_quanta_binary,\n",
        "                 #image_width_inches = 8, image_height_inches = 2,\n",
        "                 cell_num_shades = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9VDbjTAVGrP"
      },
      "outputs": [],
      "source": [
        "# Show ALL useful nodes with their algorithm sub-task tags (if any)\n",
        "show_quanta_map( \"Maths Purpose All Nodes\", qt.QType.IMPACT, \"\", qt.get_quanta_algo,\n",
        "                 #image_width_inches = 8, image_height_inches = 4,\n",
        "                 cell_num_shades = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_m3qiWYrjRJ"
      },
      "source": [
        "# Part 24: Show known quanta per answer digit\n",
        "\n",
        "Each of the late positions are soley focused on calculating one answer digit. Show the data have we collected on late answer digit.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT8zKdHz1-fZ"
      },
      "outputs": [],
      "source": [
        "for position in range(cfg.num_question_positions + 1, cfg.n_ctx - 1):\n",
        "  print(\"Position:\", position)\n",
        "\n",
        "  # Calculate a table of the known quanta for the specified position for each late token position\n",
        "  qt.calc_maths_quanta_for_position_nodes(cfg, position)\n",
        "\n",
        "  qt.save_plt_to_file(cfg=cfg, full_title=\"Quanta At \"+ qt.position_name(position))\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 25: Compare ST and SC\n",
        "The sub-tasks ST and SC are similar: They both take Dn,D'n inputs (10x10) and generate \"carry one\" outputs. They differ in that ST occurs in early tokens and has tri-state output, whereas SC occurs in late tokens and has bi-state output. For a sample mixed model, this figure shows PCA results comparing ST and SC output for A2 and A3."
      ],
      "metadata": {
        "id": "jux5CWRZ7XDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.perc_add > 0 and cfg.n_layers >= 2:\n",
        "    a2st = cfg.useful_nodes.get_node_by_tag(qt.QType.ALGO.value, \"A2.ST\")\n",
        "    a2sc = cfg.useful_nodes.get_node_by_tag(qt.QType.ALGO.value, \"A2.SC\")\n",
        "    a3st = cfg.useful_nodes.get_node_by_tag(qt.QType.ALGO.value, \"A3.ST\")\n",
        "    a3sc = cfg.useful_nodes.get_node_by_tag(qt.QType.ALGO.value, \"A3.SC\")\n",
        "\n",
        "    # For all nodes, the attention head output may be transformed by the MLP layer. The images below do NOT show this.\n",
        "    qt.manual_nodes_pca(cfg, qt.MathsToken.PLUS,\n",
        "        [[a2st.position, a2st.layer, a2st.num, 2], # A2.ST Trigram needed\n",
        "        [a2sc.position, a2sc.layer, a2sc.num, 2],  # A2.SC Bigram needed (Trigram superset okay)\n",
        "        [a3st.position, a3st.layer, a3st.num, 3],  # A3.ST Trigram needed\n",
        "        [a3sc.position, a2sc.layer, a2sc.num, 3]]) # A3.SC Bigram needed (Trigram superset okay)\n"
      ],
      "metadata": {
        "id": "sp54DotP7aLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQhheXmLTUfc"
      },
      "source": [
        "# Part 26: Save useful nodes with behaviour and algorithm tags to JSON file\n",
        "\n",
        "Show a list of the nodes that have proved useful in calculations, together with data on the nodes behavior and algorithmic purposes.\n",
        "Save the data to a Colab temporary JSON file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRW6tiM8hDOB"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.print_node_tags(qt.QType.ALGO.value, \"\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktlEftAOyEd"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list with maths tags to a temporary CoLab file in JSON format\n",
        "print( \"Saving useful node list with maths tags:\", main_fname_maths_json)\n",
        "cfg.useful_nodes.save_nodes(main_fname_maths_json, qt.QType.ALGO.value)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uG2gZSoSJD5C",
        "pTd3nmsMJV5T",
        "P8RfHXneJw6n",
        "tz6rUaYvjOcE",
        "ZHiJhch4KCej",
        "-KJhCxFtNKfm",
        "qS_zUfwrXl6P",
        "D6FwJW0tv4Nf",
        "904WBkTOLg_5",
        "Rw-Wteh-JBd6",
        "3BmQHiLALp-3",
        "Rbiau9foMp3h",
        "jIu3Pr9CMx3l",
        "jFcCpfmKwlAH",
        "IVkOJRmPvPms",
        "Fgg43Sqe8oF1",
        "Z5DMV3I_25ST",
        "Iosx5zE_macF",
        "ThzFD1FxBXg8",
        "3jOBIUDzRrGz",
        "nVyItdhmhIn8",
        "sJo37Qg2ZQpJ",
        "Agw7np7gZmgP",
        "ztfj5n8FjYNt",
        "GOVsZIpoQN_r",
        "UF4AjvfG2QDy",
        "fdWYQKi1Sr3M",
        "LQhheXmLTUfc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}