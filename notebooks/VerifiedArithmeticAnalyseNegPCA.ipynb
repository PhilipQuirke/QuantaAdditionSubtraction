{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG2gZSoSJD5C"
   },
   "source": [
    "# Analyze Attentio Head L0H0\n",
    "\n",
    "This CoLab analyses a Transformer model that performs integer addition, subtraction and multiplication e.g. 133357+182243=+0315600, 123450-345670=-0123230 and 000345*000823=+283935. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/PhilipQuirke/verified_transformers/blob/main/assets/ins1_mix_d6_l3_h4_t40K_s372001MathsPurposePerNode.svg\n",
    "https://github.com/PhilipQuirke/verified_transformers/blob/main/assets/ins1_mix_d6_l3_h4_t40K_s372001QuantaAtP18.svg\n",
    "https://github.com/PhilipQuirke/verified_transformers/blob/main/assets/Hypothesis2_A2_Calc.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTd3nmsMJV5T"
   },
   "source": [
    "# Part 0: Import libraries\n",
    "Imports standard libraries.\n",
    "\n",
    "Imports \"verified_transformer\" public library as \"qt\". This library is specific to this CoLab's \"QuantaTool\" approach to transformer analysis. Refer to [README.md](https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCdmr6-_Jkzi"
   },
   "outputs": [],
   "source": [
    "DEVELOPMENT_MODE = True\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    !pip install matplotlib\n",
    "\n",
    "    !pip install kaleido\n",
    "    !pip install transformer_lens\n",
    "    !pip install torchtyping\n",
    "    !pip install transformers\n",
    "\n",
    "    !pip install numpy\n",
    "    !pip install scikit-learn\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "    def setup_jupyter(install_libraries=False):\n",
    "        if install_libraries:\n",
    "            !pip install matplotlib==3.8.4\n",
    "            !pip install kaleido==0.2.1\n",
    "            !pip install transformer_lens==1.15.0\n",
    "            !pip install torchtyping==0.1.4\n",
    "            !pip install transformers==4.39.3\n",
    "\n",
    "            !pip install numpy==1.26.4\n",
    "            !pip install plotly==5.20.0\n",
    "            !pip install pytest==8.1.1\n",
    "            !pip install scikit-learn==1.4.1.post1\n",
    "\n",
    "        print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        ipython = get_ipython()\n",
    "        # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "        ipython.magic(\"load_ext autoreload\")\n",
    "        ipython.magic(\"autoreload 2\")\n",
    "\n",
    "    # setup_jupyter(install_libraries=True)   # Uncomment if you need to install libraries in notebook.\n",
    "    setup_jupyter(install_libraries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up2QLAZLJnG9"
   },
   "outputs": [],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import kaleido\n",
    "import plotly.io as pio\n",
    "\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve-TndERJoaJ"
   },
   "outputs": [],
   "source": [
    "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.title.font.size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6zOEFryJqGN"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6TE7A9SxySA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8VQ4e0QJsIB"
   },
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUt2mMzFj7eA"
   },
   "outputs": [],
   "source": [
    "# Import Principal Component Analysis (PCA) library\n",
    "use_pca = True\n",
    "try:\n",
    "  from sklearn.decomposition import PCA\n",
    "except Exception as e:\n",
    "  print(\"pca import failed with exception:\", e)\n",
    "  use_pca = False\n",
    "\n",
    "  # Sometimes version conflicts means the PCA library does not import. This workaround partially fixes the issue\n",
    "  !pip install --upgrade numpy\n",
    "  !pip install --upgrade scikit-learn\n",
    "\n",
    "  # To complete workaround, now select menu option \"Runtime > Restart session and Run all\".\n",
    "  stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwekwkrdI6SX"
   },
   "outputs": [],
   "source": [
    "! pip uninstall QuantaTools -y || true   # Ensure a clean install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2P3cndolKDM"
   },
   "outputs": [],
   "source": [
    "# Refer https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md\n",
    "!pip install --upgrade git+https://github.com/PhilipQuirke/verified_transformers.git@amir/analyze_pca_outputs_on_l0h1  # Specify @branch if testing a specific branch\n",
    "import QuantaTools as qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldGPkaokJQM5"
   },
   "source": [
    "# Part 1A: Configuration\n",
    "\n",
    "Which existing model do we want to analyse?\n",
    "\n",
    "The existing model weightings created by the sister Colab [VerifiedArithmeticTrain](https://github.com/PhilipQuirke/transformer-maths/blob/main/assets/VerifiedArithmeticTrain.ipynb) are loaded from HuggingFace (in Part 5). Refer https://github.com/PhilipQuirke/verified_transformers/blob/main/README.md for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmjGdFcdJat3"
   },
   "outputs": [],
   "source": [
    "# Singleton QuantaTool \"main\" configuration class. MathsConfig is derived from the chain AlgoConfig > UsefulConfig > ModelConfig\n",
    "cfg = qt.MathsConfig()\n",
    "\n",
    "# Singleton QuantaTool \"ablation intervention\" configuration class\n",
    "acfg = qt.acfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1DXZQ2E6yAi"
   },
   "outputs": [],
   "source": [
    "cfg.model_name = \"ins1_mix_d6_l3_h4_t40K_s372001\"  # AvgFinalLoss=1.7e-08. Accurate on 1M Qs\n",
    "cfg.perc_sub = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_IIpX2H2tNe"
   },
   "source": [
    "# Part 1B: Configuration: Input and Output file names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5BiELcf53ms"
   },
   "outputs": [],
   "source": [
    "# Needed when user changes model_name and reruns this Colab a second time\n",
    "cfg.reset_useful()\n",
    "cfg.reset_algo()\n",
    "cfg.initialize_maths_token_positions()\n",
    "acfg.reset_ablate()\n",
    "\n",
    "if cfg.model_name != \"\":\n",
    "  # Update cfg member data n_digits, n_layers, n_heads, n_training_steps from model_name\n",
    "  cfg.parse_model_name()\n",
    "\n",
    "  if cfg.model_name.startswith(\"ins1_mix_d6_l3\") :\n",
    "    if cfg.training_seed == 372001:\n",
    "      # Mixed model initialised with add_d6_l2_h3_t15K.pth.\n",
    "      cfg.insert_n_training_steps = 15000\n",
    "    else:\n",
    "      # Mixed model initialised with add_d6_l2_h3_t20K.pth.\n",
    "      cfg.insert_n_training_steps = 20000\n",
    "\n",
    "  cfg.batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.op_config_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0DJkn5l2gq3"
   },
   "outputs": [],
   "source": [
    "main_fname = cfg.file_config_prefix()\n",
    "main_fname_pth = main_fname + '.pth'\n",
    "main_fname_behavior_json = main_fname + '_behavior.json'\n",
    "main_fname_algorithm_json = main_fname + '_algorithm.json'\n",
    "\n",
    "def print_config():\n",
    "  print(\"%Add=\", cfg.perc_add(), \"%Sub=\", cfg.perc_sub, \"%Mult=\", cfg.perc_mult, \"InsertMode=\", cfg.insert_mode, \"File=\", main_fname)\n",
    "\n",
    "print_config()\n",
    "print(\"weight_decay=\", cfg.weight_decay, \"lr=\", cfg.lr, \"batch_size=\", cfg.batch_size)\n",
    "print('Main model will be read from HuggingLab file', main_fname_pth)\n",
    "print('Main model behavior analysis tags will save to Colab temporary file', main_fname_behavior_json)\n",
    "print('Main model algorithm analysis tags will save to Colab temporary file', main_fname_algorithm_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8RfHXneJw6n"
   },
   "source": [
    "# Part 3A: Set Up: Vocabulary / Embedding / Unembedding\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSsQ8EpC1g7M"
   },
   "outputs": [],
   "source": [
    "main_fname_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeObQk2kzAv7"
   },
   "outputs": [],
   "source": [
    "qt.set_maths_vocabulary(cfg)\n",
    "qt.set_maths_question_meanings(cfg)\n",
    "print(cfg.token_position_meanings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz6rUaYvjOcE"
   },
   "source": [
    "# Part 3B: Set Up: Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lA16Nb2PJ7MB"
   },
   "outputs": [],
   "source": [
    "# Transformer creation\n",
    "\n",
    "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
    "ht_cfg = HookedTransformerConfig(\n",
    "    n_layers = cfg.n_layers,\n",
    "    n_heads = cfg.n_heads,\n",
    "    d_model = cfg.d_model,\n",
    "    d_head = cfg.d_head,\n",
    "    d_mlp = cfg.d_mlp(),\n",
    "    act_fn = cfg.act_fn,\n",
    "    normalization_type = 'LN',\n",
    "    d_vocab = cfg.d_vocab,\n",
    "    d_vocab_out = cfg.d_vocab,\n",
    "    n_ctx = cfg.n_ctx(),\n",
    "    init_weights = True,\n",
    "    device = \"cuda\",\n",
    "    seed = cfg.training_seed,\n",
    ")\n",
    "\n",
    "cfg.main_model = HookedTransformer(ht_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHiJhch4KCej"
   },
   "source": [
    "# Part 4: Set Up: Loss Function & Data Generator\n",
    "This maths loss function and data generator are imported from QuantaTools as logits_to_tokens_loss, loss_fn, maths_data_generator_core and maths_data_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqzljhQ4KJU5"
   },
   "outputs": [],
   "source": [
    "# Define \"iterator\" maths \"questions\" data generator function. Invoked using next().\n",
    "ds = qt.maths_data_generator( cfg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtmioT1THbJA"
   },
   "outputs": [],
   "source": [
    "# Generate sample data generator (unit test)\n",
    "print(next(ds)[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KJhCxFtNKfm"
   },
   "source": [
    "# Part 5: Set Up: Load Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD4_HR5h1g7N"
   },
   "outputs": [],
   "source": [
    "main_fname_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRMkB_8GNRc0"
   },
   "outputs": [],
   "source": [
    "main_repo_name=\"PhilipQuirke/VerifiedArithmetic\"\n",
    "print(\"Loading model from HuggingFace\", main_repo_name, main_fname_pth)\n",
    "\n",
    "cfg.main_model.load_state_dict(utils.download_file_from_hf(repo_name=main_repo_name, file_name=main_fname_pth, force_is_torch=True))\n",
    "cfg.main_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-yFKiDllqEf"
   },
   "source": [
    "# Part 6A: Look at Math Purpose Per Node\n",
    "\n",
    "* Rerunning the main notebook, and looking at the purpose of each node.\n",
    "* In particular, we can see that L0HO impacts performance on A1.SC, A1.MB and A2.NB. We want to more formally verify the polysemantic behavior of this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'ins1_mix_d6_l3_h4_t40K_s372001MathsPurposePerNode.pdf'\n",
    "\n",
    "# Display the PDF using an inline frame\n",
    "display(IFrame(pdf_path, width=700, height=575))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbiau9foMp3h"
   },
   "source": [
    "# Part 7: Results: Manual interpretation of PCA results\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful technique that aids in mechanistic interpretability by simplifying complex datasets into principal components that capture the most significant variance within the data.\n",
    "\n",
    "This library uses PCA to help understand the purpose of individual useful nodes. For more background refer https://github.com/PhilipQuirke/verified_transformers/blob/main/pca.md\n",
    "\n",
    "If an attention head and an answer digit An gives an interpretable response (2 or 3 distinct output clusters) on 3 groups of questions aligned to T8, T9 and T10 definitions, then plot the response and add a PCA tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuantaTools.maths_tools.maths_test_questions.tricase_test_questions_generator import CustomTriclassConfig, OperatorQTypeNumber, TOTAL_TRICASE_QUESTIONS\n",
    "from QuantaTools.maths_tools.maths_constants import MathsToken\n",
    "from QuantaTools.maths_tools.maths_complexity import SimpleQuestionDescriptor\n",
    "\n",
    "from QuantaTools.quanta_constants import QType\n",
    "\n",
    "from QuantaTools import make_maths_tricase_questions_customized, make_maths_tricase_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuantaTools.maths_tools.maths_test_questions.tricase_test_questions_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QType\n",
    "TriCaseBehavior\n",
    "MathsToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DigitOperatorQTypeTricase(digit=1, operator=11, qtype=QType.MATH_SUB, test_case=TriCaseBehavior.MT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 A: Question configurations for classes of questions.\n",
    "Set up question configs for Neg only, Add Only, Sub Only and mixture configurations.\n",
    "Will help with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sub_and_neg_config = CustomTriclassConfig((\n",
    "    OperatorQTypeNumber(MathsToken.PLUS, QType.MATH_ADD, batch_size),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_SUB, batch_size),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_NEG, batch_size)\n",
    "))\n",
    "\n",
    "add_only_config = CustomTriclassConfig((\n",
    "    OperatorQTypeNumber(MathsToken.PLUS, QType.MATH_ADD, batch_size),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_SUB, 0),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_NEG, 0)\n",
    "))\n",
    "\n",
    "sub_only_config = CustomTriclassConfig((\n",
    "    OperatorQTypeNumber(MathsToken.PLUS, QType.MATH_ADD, 0),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_SUB, batch_size),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_NEG, 0)\n",
    "))\n",
    "\n",
    "neg_only_config = CustomTriclassConfig((\n",
    "    OperatorQTypeNumber(MathsToken.PLUS, QType.MATH_ADD, 0),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_SUB, 0),\n",
    "    OperatorQTypeNumber(MathsToken.MINUS, QType.MATH_NEG, batch_size)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 B: PCA the output of P18L0H0 for ADD, SUB, NEG questions with no SC/MB/NB features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "add_only_questions = make_maths_tricase_questions_customized(cfg, add_only_config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_only_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_single_tricase_question(cfg=None, test_digit=1, test_case=TriCaseBehavior.MT3, operation=MathsToken.MINUS, qtype=QType.MATH_NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = DigitOperatorQTypeTricase(digit=1, operator=maths_tokens_to_names[MathsToken.MINUS], qtype=QType.MATH_NEG, test_case=TriCaseBehavior.MT3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = cfg.customized_tricase_questions_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(item) for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "\n",
    "target_cases = [TriCaseBehavior.MT3]\n",
    "qtype=QType.MATH_NEG            \n",
    "num_questions = 300\n",
    "local_num_questions = int(num_questions / len(target_cases))\n",
    "operator = MathsToken.MINUS\n",
    "\n",
    "\n",
    "answer_digit = 0\n",
    "for test_case in target_cases:\n",
    "    all_questions = make_tricase_questions(\n",
    "            cfg, test_digit=answer_digit, test_case=test_case, operation=operator, qtype=qtype, num_questions=local_num_questions\n",
    "    )\n",
    "    key = DigitOperatorQTypeTricase(answer_digit, maths_tokens_to_names[operator], qtype, test_case)\n",
    "\n",
    "    my_dict[key] = all_questions\n",
    "\n",
    "questions_created = [len(my_dict.get(\n",
    "            DigitOperatorQTypeTricase(answer_digit, maths_tokens_to_names[operator], qtype, test_case), [])) for test_case in TriCaseBehavior\n",
    "]\n",
    "\n",
    "questions_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_questions = make_maths_tricase_questions_customized(cfg, add_sub_and_neg_config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.customized_tricase_questions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_only_questions = make_maths_tricase_questions_customized(cfg, neg_only_config, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_only_questions_no_mb_or_nb = DigitOperatorQTypeTricase(digit=0, operator=MathsToken.MINUS, qtype=QType.MATH_NEG, tricase=TriCaseBehavior.ST8)\n",
    "neg_only_questions_no_mb_or_nb = DigitOperatorQTypeTricase(digit=0, operator=MathsToken.MINUS, qtype=QType.MATH_NEG, tricase=TriCaseBehavior.ST8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_only_questions[DigitOperatorQTypeTricase(digit=5, operator=11, qtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = make_maths_tricase_questions_customized(cfg, add_sub_and_neg_config)\n",
    "\n",
    "questions = make_maths_tricase_questions_customized(cfg, add_only_config)\n",
    "\n",
    "questions = make_maths_tricase_questions_customized(cfg, sub_only_config)\n",
    "\n",
    "questions = make_maths_tricase_questions_customized(cfg, neg_only_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mappings = {'T8':  'red', 'T9': 'green', 'T10': 'blue'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxlgEAPV1g7W"
   },
   "outputs": [],
   "source": [
    "# Create a cache of sample maths questions based on the T8, T9, T10 categorisation in cfg.tricase_questions_dict\n",
    "qt.make_maths_tricase_questions(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cfg.main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uG2gZSoSJD5C",
    "pTd3nmsMJV5T",
    "F_IIpX2H2tNe",
    "P8RfHXneJw6n",
    "tz6rUaYvjOcE",
    "ZHiJhch4KCej",
    "-KJhCxFtNKfm",
    "I-yFKiDllqEf",
    "D6FwJW0tv4Nf",
    "2PjaQvhhayUL",
    "nXBYdxj-jLZc",
    "ZmsGWUbILYin",
    "904WBkTOLg_5",
    "Rw-Wteh-JBd6",
    "3BmQHiLALp-3",
    "jFcCpfmKwlAH",
    "IVkOJRmPvPms",
    "Iosx5zE_macF",
    "3jOBIUDzRrGz",
    "nVyItdhmhIn8",
    "GOVsZIpoQN_r",
    "D74GWTY3aKhm",
    "dZk-BAE2n5kZ",
    "ig0eUEDN5XmG"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
