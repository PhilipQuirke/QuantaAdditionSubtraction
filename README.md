# verified_transformers
Tool used to verify accuracy of transformer model. Contains files:

- Notebooks: Colab notebook files
  - VerifiedArithmeticTrain.ipynb: Colab used to train transformer arithmetic models. 
    - Outputs pth and json files that are (manually) stored on HuggingFace
  - VerifiedArithmeticAnalyse.ipynb: Colab used to analyze the behavior and algorithm of transformer arithmetic models
    - Inputs pth files (generated above) from HuggingFace
    - Outputs *_behavior and *_algorithm json files that are (manually) stored on HuggingFace 
  - Accurate_Math_Train.ipynb: Deprecated. Predecessor of VerifiedArithmeticTrain. Associated with https://arxiv.org/abs/2402.02619 
  - Accurate_Math_Analyse.ipynb: Deprecated. Predecessor of VerifiedArithmeticAnalyse associated with https://arxiv.org/abs/2402.02619

- QuantaTools: Python library code
  - model_*.py: Contains the configuration of the transformer model being trained/analysed
  - useful_*.py: Contains data on the useful token positions and useful nodes (attention heads and MLP neurons) that the model uses in predictions
  - quanta_*.py: Contains categorisations of model behavior (aka quanta), with ways to detect and graph them 
  - ablate_*.py: Contains ways to "intervention ablate" the model and detect the impact of the ablation
  - algo_*.py: Contains tools to support declaring and validating a model algorithm
  - maths_*.py: Contains specializations of the above specific to arithmetic (addition and subtraction) transformer models
          
- Tests: Unit tests 
          
HuggingFace permanently stores the output files generated by the 'train' and 'analyse' CoLabs:
  - VerifiedArithmeticTrain/Analyse files are stored at https://huggingface.co/PhilipQuirke/Accurate6DigitSubtraction covering these models:
    - add_**d5_l1**_h3_t30K: **5-digit, 1-layer**, addition model. Inaccurate. Can only predict S0, S1 and S2 complexity questions
    - add_d5_**l2**_h3_t15K: 5-digit, **2-layers**, 3-attention-head addition model trained for 15000 epochs. Accurate 
    - add_**d6**_l2_h3_t15K: **6-digit**, 2-layers, 3-attention-head addition model trained for 15000 epochs. Accurate
    - **sub**_d6_l2_h3_t30K: 6-digit, 2-layers, 3-attention-head **subtraction** model trained for 30000 epochs. Inaccurate
    - **mix**_d6_l3_h4_t40K: 6-digit, **3-layers, 4-attention-head mixed** (addition and subtraction) model trained for 40000. Accurate (training loss = 8e-09)
    - **ins1**_mix_d6_l3_h4_t40K: 6-digit, 3-layers, 4-attention-head mixed **initialise with addition model**. Accurate (handles 1m Qs for Add and Sub)
    - **ins2**_mix_d6_l4_h4_t40K"  # 6 digit addition / subtraction model. Initialised with addition model. Reset useful heads every 100 epochs. AvgFinalLoss=7e-09. Fails 1m Qs
    - **ins3**_mix_d6_l4_h3_t40K"  # 6 digit addition / sub 
  - Accurate_Math_Train/Analyse (deprecated) files are stored at https://huggingface.co/PhilipQuirke/Accurate6DigitSubtraction
